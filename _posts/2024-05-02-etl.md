---
title: Data Extraction, Transformation, and Loading (ETL)
layout: post
---

<span class="firstcharacter">D</span>ata extraction, transformation, and loading (ETL) is a process in which data is extracted from a source, transformed into a format that is suitable for analysis, and then loaded into a destination. ETL is a critical step in the data pipeline because it ensures that the data is clean, consistent, and ready for analysis.

We will use the following tools to do ETL:

- postgres: postgres is an open-source relational database management system that is widely used for storing and managing data. We will use postgres to store the data that we extract, transform, and load.
- dbt: dbt is a command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively. dbt allows you to write SQL queries to transform your data and then run those queries in a reproducible and scalable way.
- Apache Airflow: Apache Airflow is an open-source platform that allows you to programmatically author, schedule, and monitor workflows. We will use Apache Airflow to orchestrate our ETL process and ensure that it runs smoothly.
- basic cloud services knowledge: we will use basic cloud services such as AWS S3, Google Cloud Storage, or Azure Blob Storage or DigitalOcean Spaces to store the data that we extract, transform, and load.


## Step 0: upload data to the cloud

As a machine learning engineer or data scientist, most of the time you will be teaming up with data base engineers or data engineers to build data pipelines. Very often, data warehouse is given to you by the data engineers. For big firms, raw data is normally streamed in and then stored in a data warehouse. For small firms, raw data is normally stored in a cloud storage service such as AWS S3, Google Cloud Storage, or Azure Blob Storage or DigitalOcean Spaces.

Again, <span class="exploration"> most of time you do not need to know how to upload big data to the cloud. </span> However, it is good to know how to do it. For a large data file like `parquet` or `csv`, they are normally stored in a s3 bucket. In this case, I recommend you to use either `awscli` or `s3cmd` to upload the data to the cloud. Here are two references you can use to learn how to use `s3cmd`:

- [s3cmd with DigitalOcean Spaces](https://docs.digitalocean.com/products/spaces/reference/s3cmd-usage/){:target="_blank"}
- [s3cmd with AWS S3](https://simplebackups.com/blog/mastering-s3-sync-s3cmd-rclone-ultimate-guide/){:target="_blank"}
- [s3cmd with Google Cloud Storage](https://addshore.com/2022/09/google-cloud-storage-upload-with-s3cmd/){:target="_blank"}