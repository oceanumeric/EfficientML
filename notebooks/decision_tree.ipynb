{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Recently, many kaggle competition winners have used ensemble methods like Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost, etc. These methods are very powerful and can achieve very high accuracy, especially when you have lots of **category features**. \n",
    "\n",
    "To understand how these ensemble methods work, we need to understand the basic building block of these methods, which is the Decision Tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tree has nodes and edges. The nodes are the decision points, and the edges are the outcomes of the decision. The tree starts with a root node and ends with leaf nodes. The root node is the first decision point, and the leaf nodes are the final outcomes.\n",
    "\n",
    "We can build a decision tree recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth: int):\n",
    "        assert max_depth >= 0  # max_depth must be a positive integer\n",
    "        self.max_depth = max_depth\n",
    "        if max_depth > 0:\n",
    "            self.left = DecisionTree(max_depth - 1)\n",
    "            self.right = DecisionTree(max_depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTree(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max_depth, t.left.max_depth, t.right.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to get an apple out of a basket of fruits. You are actually doing classification:\n",
    "\n",
    "- check the shape of the fruit\n",
    "    - if it is round, check the color\n",
    "        - if it is red, it is an apple\n",
    "        - if it is not red, it is not an apple\n",
    "    - if it is not round, it is not an apple\n",
    "\n",
    "In reality, our mind works like a decision tree in nanoseconds.\n",
    "\n",
    "In machine learning, we usually have a dataset with many features and a target variable. We can build a decision tree to predict the target variable based on the features.\n",
    "\n",
    "Suppose we have the following table:\n",
    "\n",
    "| feature_value| color|\n",
    "|--------------|------|\n",
    "| 20       | red  |\n",
    "| 30        | red  |\n",
    "| 40       | red |\n",
    "| 50        | green|\n",
    "| 60        | green|\n",
    "| 70        | green|\n",
    "\n",
    "We can build a decision tree to predict the color based on the feature value. The above example gives the extreme scenario where the feature value is strongly associated with the color. Meaning, whenever the feature value is less than 50, the color is red, and whenever the feature value is greater than 50, the color is green.\n",
    "\n",
    "Our goal is to come up a machenism to build a decision tree based on the data, so that we can predict the target variable based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series,\n",
    "                 min_samples_leaf: int = 5,\n",
    "                 max_depth: int = 6,\n",
    "                 idxs: np.ndarray = None):\n",
    "        assert max_depth >= 0\n",
    "        assert min_samples_leaf > 0\n",
    "        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        if idxs is None:\n",
    "            idxs = np.arange(len(y))\n",
    "        self.X, self.y, self.idxs = X, y, idxs\n",
    "        self.n, self.m = len(idxs), X.shape[1]\n",
    "        # value of the node is the mean of y\n",
    "        self.value = np.mean(y[idxs])\n",
    "        self.best_score_so_far = float('inf') # initial loss before split finding\n",
    "        \n",
    "        if self.max_depth > 0:\n",
    "            self._maybe_insert_child_nodes()\n",
    "    \n",
    "    def _maybe_insert_child_nodes(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019907 -0.017646  \n",
       "1 -0.039493 -0.068332 -0.092204  \n",
       "2 -0.002592  0.002861 -0.025930  \n",
       "3  0.034309  0.022688 -0.009362  \n",
       "4 -0.002592 -0.031988 -0.046641  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151.0\n",
       "1     75.0\n",
       "2    141.0\n",
       "3    206.0\n",
       "4    135.0\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTree(X, y, min_samples_leaf=5, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Child Nodes\n",
    "\n",
    "When we are searching for an apple, every time we make a decision, we are inserting a child node. For instance, when we check the shape of the fruit, we are inserting a child node. When we check the color of the fruit, we are inserting another child node.\n",
    "\n",
    "Therefore, we can build a decision tree by inserting child nodes based on the feature values. Read the following code first and then I will explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_insert_child_nodes(self):\n",
    "        for j in range(self.m):\n",
    "            # there exist a better split for feature j\n",
    "            # such as value < 50 for color red in our example\n",
    "            # we will implement this method later\n",
    "            self._find_better_split(j)\n",
    "        if self.is_leaf:\n",
    "            # no need to create child nodes\n",
    "            return\n",
    "        x = self.X.values[self.idxs, self.split_feature_idx]\n",
    "        # get the indexes of the samples that are less than \n",
    "        # or equal to the split value\n",
    "        # this step is to sort the samples to the left and right nodes\n",
    "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
    "        right_idx = np.nonzero(x > self.threshold)[0]\n",
    "        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[left_idx])\n",
    "        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[right_idx])\n",
    "        \n",
    "def _find_better_split(self, feature_idx):\n",
    "    pass\n",
    "\n",
    "@property\n",
    "def is_leaf(self):\n",
    "    return self.best_score_so_far == float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero([1, 2] < [4, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree._maybe_insert_child_nodes = _maybe_insert_child_nodes\n",
    "DecisionTree._find_better_split = _find_better_split\n",
    "DecisionTree.is_leaf = is_leaf\n",
    "t = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Finding\n",
    "\n",
    "Now, let's see the following table with two features and a target variable.\n",
    "\n",
    "| feature1 | feature2 | target |\n",
    "|----------|----------|--------|\n",
    "| 20       | A       | red    |\n",
    "| 30       | B      | red    |\n",
    "| 40       | A      | red    |\n",
    "| 50       | A       | green  |\n",
    "| 60       | B     | green  |\n",
    "| 70       | B     | green  |\n",
    "\n",
    "When any target value is determined by the will of the GOD, there might be some relationship between the features and the target variable. Suppose we have a set of features $(f_1, f_2, f_3, ..., f_n)$ and a target variable $y$.\n",
    "\n",
    "Those features could be equally important, or some of them could be more important than others, or some combination of features could be more important than others. For instance, we can have $(f_1, f_5, f_n)$ as the most important features. And this combination then will then work together with other features to determine the target variable.\n",
    "\n",
    "To determine whether the combination of features is important or not, we need a metric to measure the split.\n",
    "\n",
    "For classification, we will use `majority voting` to determine the target variable. For instance, if we have 3 reds and 2 greens, the target variable will be red. If we have 2 reds and 3 greens, the target variable will be green.\n",
    "\n",
    "For the regression problem, we will use the `mean` of the target variable. This means we simply take the average of the target variable as the best guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we will train a regression tree:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the actual target value and $\\hat{y_i}$ is the predicted target value. For a given node, we can replace $\\hat{y_i}$ with the mean of the target values in that node.\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "where $\\bar{y}$ is the mean of the target values in that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will expand the above equation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L & = \\sum_{i=1}^{n} (y_i^2 - 2y_i\\bar{y} + \\bar{y}^2) \\\\\n",
    "& = \\sum_{i=1}^{n} y_i^2 - 2\\bar{y}\\sum_{i=1}^{n} y_i + n\\bar{y}^2 \\\\\n",
    "& = \\sum_{i=1}^{n} y_i^2 - 2n\\bar{y}^2 + n\\bar{y}^2 \\\\\n",
    "& = \\sum_{i=1}^{n} y_i^2 - n\\bar{y}^2 \\\\\n",
    "& = \\sum_{i=1}^{n} y_i^2 - n\\left(\\frac{\\sum_{i=1}^{n} y_i}{n}\\right)^2 \\\\\n",
    "& = \\sum_{i=1}^{n} y_i^2 - \\frac{1}{n} \\left (\\sum_{i=1}^{n} y_i \\right)^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let’s work out a simple expression for the loss reduction from a given split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong connection between squared error loss and variance. The variance of a set of values is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ is the mean of the values. We can rewrite the variance as:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\frac{1}{n} \\left(\\sum_{i=1}^{n} x_i \\right)^2\n",
    "$$\n",
    "\n",
    "Therefore, when we split the data into two groups, the total variance is the sum of the variances of the two groups. The variance reduction is the difference between the total variance and the sum of the variances of the two groups. We will only split the data when the variance reduction is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for Variance Reduction\n",
    "\n",
    "Let $I$ be the set of $n$ data instance in the current node, and let $I_L$ and $I_R$ be the set of data instances in the left and right child nodes after the split.\n",
    "\n",
    "The loss before the split is:\n",
    "\n",
    "$$\n",
    "L_{\\text{before}} = \\sum_{i \\in I} y_i^2 - \\frac{1}{n} \\left(\\sum_{i \\in I} y_i \\right)^2\n",
    "$$\n",
    "\n",
    "The loss after the split is:\n",
    "\n",
    "$$\n",
    "L_{\\text{after}} = I_L + I_R =  \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left(\\sum_{i \\in I_L} y_i \\right)^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left(\\sum_{i \\in I_R} y_i \\right)^2\n",
    "$$\n",
    "\n",
    "The reduction in loss is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Reduction} & = L_{\\text{after}} - L_{\\text{before}} \\\\\n",
    "& = \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left(\\sum_{i \\in I_L} y_i \\right)^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left(\\sum_{i \\in I_R} y_i \\right)^2 - \\sum_{i \\in I} y_i^2 + \\frac{1}{n} \\left(\\sum_{i \\in I} y_i \\right)^2 \\\\\n",
    "& = - \\frac{1}{n_L} \\left(\\sum_{i \\in I_L} y_i \\right)^2 - \\frac{1}{n_R} \\left(\\sum_{i \\in I_R} y_i \\right)^2 + \\frac{1}{n} \\left(\\sum_{i \\in I} y_i \\right)^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score(self, sum_y_left, n_left, sum_y_right, n_right, sum_y, n):\n",
    "    score = -(sum_y_left ** 2) / n_left - (sum_y_right ** 2) / n_right + (sum_y ** 2) / n\n",
    "    return score\n",
    "\n",
    "def _find_better_split(self, feature_idx):\n",
    "    x = self.X.values[self.idxs, feature_idx]\n",
    "    y = self.y[self.idxs]\n",
    "    # we sort the samples by the feature values\n",
    "    # this will help us to pin down the threshold values\n",
    "    sort_idx = np.argsort(x)\n",
    "    sort_y, sort_x = y[sort_idx], x[sort_idx]\n",
    "    sum_y, n = sort_y.sum(), len(sort_y)\n",
    "    sum_y_right, n_right = sum_y, n\n",
    "    sum_y_left, n_left = 0, 0\n",
    "    \n",
    "    for i in range(0, self.n - self.min_samples_leaf):\n",
    "        xi, xi_next, yi = sort_x[i], sort_x[i + 1], sort_y[i]\n",
    "        sum_y_left += yi\n",
    "        sum_y_right -= yi\n",
    "        n_left += 1\n",
    "        n_right -= 1\n",
    "        if n_left < self.min_samples_leaf or xi == xi_next:\n",
    "            # continue to search for the next split\n",
    "            continue\n",
    "        score = self._score(sum_y_left, n_left, sum_y_right, n_right, sum_y, n)\n",
    "        \n",
    "        if score < self.best_score_so_far:\n",
    "            self.best_score_so_far = score\n",
    "            self.split_feature_idx = feature_idx\n",
    "            self.threshold = (xi + xi_next) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s5', -0.0037611760063045703)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree._score = _score\n",
    "DecisionTree._find_better_split = _find_better_split\n",
    "t = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)\n",
    "X.columns[t.split_feature_idx], t.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019907 -0.017646  \n",
       "1 -0.039493 -0.068332 -0.092204  \n",
       "2 -0.002592  0.002861 -0.025930  \n",
       "3  0.034309  0.022688 -0.009362  \n",
       "4 -0.002592 -0.031988 -0.046641  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "    s = f'n: {self.n}; value: {self.value:.2f}'\n",
    "    if not self.is_leaf:\n",
    "        split_feature_name = self.X.columns[self.split_feature_idx]\n",
    "        s += f'; split_feature: {split_feature_name} <= {self.threshold:.2f}'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 442; value: 152.13; split_feature: s5 <= -0.00\n",
      "n: 218; value: 109.99; split_feature: bmi <= 0.01\n",
      "n: 171; value: 96.31\n"
     ]
    }
   ],
   "source": [
    "DecisionTree.__repr__ = __repr__\n",
    "t = DecisionTree(X, y, min_samples_leaf=5, max_depth=2)\n",
    "print(t)\n",
    "print(t.left)\n",
    "print(t.left.left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To predict the target variable, we will use the mean of the target variable in the leaf node. And then we could build up the tree recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    return np.array([self._predict_row(xi) for xi in X.values]) \n",
    "\n",
    "def _predict_row(self, xi):\n",
    "    if self.is_leaf:\n",
    "        return self.value\n",
    "    t = self.left if xi[self.split_feature_idx] <= self.threshold else self.right\n",
    "    return t._predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225.87962963,  96.30994152, 225.87962963])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree.predict = predict\n",
    "DecisionTree._predict_row = _predict_row\n",
    "t.predict(X.iloc[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series,\n",
    "                 min_samples_leaf: int = 5,\n",
    "                 max_depth: int = 6,\n",
    "                 idxs: np.ndarray = None):\n",
    "        assert max_depth >= 0\n",
    "        assert min_samples_leaf > 0\n",
    "        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        if idxs is None:\n",
    "            idxs = np.arange(len(y))\n",
    "        self.X, self.y, self.idxs = X, y, idxs\n",
    "        self.n, self.m = len(idxs), X.shape[1]\n",
    "        # value of the node is the mean of y\n",
    "        self.value = np.mean(y[idxs])\n",
    "        self.best_score_so_far = float('inf') # initial loss before split finding\n",
    "        \n",
    "        if self.max_depth > 0:\n",
    "            self._maybe_insert_child_nodes()\n",
    "            \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.best_score_so_far == float('inf')\n",
    "    \n",
    "    def _maybe_insert_child_nodes(self):\n",
    "        for j in range(self.m):\n",
    "            # there exist a better split for feature j\n",
    "            # such as value < 50 for color red in our example\n",
    "            # we will implement this method later\n",
    "            self._find_better_split(j)\n",
    "        if self.is_leaf:\n",
    "            # no need to create child nodes\n",
    "            return\n",
    "        x = self.X.values[self.idxs, self.split_feature_idx]\n",
    "        # get the indexes of the samples that are less than \n",
    "        # or equal to the split value\n",
    "        # this step is to sort the samples to the left and right nodes\n",
    "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
    "        right_idx = np.nonzero(x > self.threshold)[0]\n",
    "        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[left_idx])\n",
    "        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[right_idx])\n",
    "        \n",
    "    def _score(self, sum_y_left, n_left, sum_y_right, n_right, sum_y, n):\n",
    "        score = -(sum_y_left ** 2) / n_left - (sum_y_right ** 2) / n_right + (sum_y ** 2) / n\n",
    "        return score\n",
    "\n",
    "    def _find_better_split(self, feature_idx):\n",
    "        x = self.X.values[self.idxs, feature_idx]\n",
    "        y = self.y[self.idxs]\n",
    "        # we sort the samples by the feature values\n",
    "        # this will help us to pin down the threshold values\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y, sort_x = y[sort_idx], x[sort_idx]\n",
    "        sum_y, n = sort_y.sum(), len(sort_y)\n",
    "        sum_y_right, n_right = sum_y, n\n",
    "        sum_y_left, n_left = 0, 0\n",
    "        \n",
    "        for i in range(0, self.n - self.min_samples_leaf):\n",
    "            xi, xi_next, yi = sort_x[i], sort_x[i + 1], sort_y[i]\n",
    "            sum_y_left += yi\n",
    "            sum_y_right -= yi\n",
    "            n_left += 1\n",
    "            n_right -= 1\n",
    "            if n_left < self.min_samples_leaf or xi == xi_next:\n",
    "                # continue to search for the next split\n",
    "                continue\n",
    "            score = self._score(sum_y_left, n_left, sum_y_right, n_right, sum_y, n)\n",
    "            \n",
    "            if score < self.best_score_so_far:\n",
    "                self.best_score_so_far = score\n",
    "                self.split_feature_idx = feature_idx\n",
    "                self.threshold = (xi + xi_next) / 2\n",
    "\n",
    "                \n",
    "    def __repr__(self):\n",
    "        s = f'n: {self.n}; value: {self.value:.2f}'\n",
    "        if not self.is_leaf:\n",
    "            split_feature_name = self.X.columns[self.split_feature_idx]\n",
    "            s += f'; split_feature: {split_feature_name} <= {self.threshold:.2f}'\n",
    "        return s\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(xi) for xi in X.values]) \n",
    "\n",
    "    def _predict_row(self, xi):\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        t = self.left if xi[self.split_feature_idx] <= self.threshold else self.right\n",
    "        return t._predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from scratch MSE: 0.3988\n",
      "scikit-learn MSE: 0.3988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "max_depth = 8\n",
    "min_samples_leaf = 16\n",
    "\n",
    "tree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "pred = tree.predict(X_test)\n",
    "\n",
    "sk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "sk_tree.fit(X_train, y_train)\n",
    "sk_pred = sk_tree.predict(X_test)\n",
    "\n",
    "print(f'from scratch MSE: {mean_squared_error(y_test, pred):0.4f}')\n",
    "print(f'scikit-learn MSE: {mean_squared_error(y_test, sk_pred):0.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
